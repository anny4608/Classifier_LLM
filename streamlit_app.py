import streamlit as st
import os

from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings


# --- Environment Variable Check ---
# Using st.secrets for Streamlit deployment, with a fallback for local development
try:
    # This will work when deployed on Streamlit Community Cloud
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (FileNotFoundError, KeyError):
    # For local development, it will look for an environment variable
    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')

if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEY not found. Please set it in your Streamlit secrets or as an environment variable.")
    st.stop()

# --- Load FAISS Vector Store and Retriever ---
@st.cache_resource
def load_retriever():
    FAISS_INDEX_PATH = "faiss_index"
    if not os.path.exists(FAISS_INDEX_PATH):
        st.error(f"FAISS index directory '{FAISS_INDEX_PATH}' not found. Please ensure the index is in the correct location.")
        st.stop()

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        return vector_store.as_retriever()
    except Exception as e:
        st.error(f"Failed to load FAISS index: {e}")
        st.stop()

# --- LLM and QA Chain Setup ---
@st.cache_resource
def load_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.1
    )

# --- Main Function to get answer ---
def get_answer(question: str, retriever, llm):
    """
    Answers a user question using RAG with technical guidance.
    """
    # 1. Retrieve relevant documents
    docs = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in docs])

    # 2. Define technical guidance
    classification_key_terms = [
        "SVM (æ”¯æŒå‘é‡æ©Ÿ)ï¼šæ ¸å¿ƒæ¦‚å¿µæ˜¯å°‹æ‰¾ä¸€å€‹æœ€å¤§é‚Šç•Œ (Maximum Margin) çš„è¶…å¹³é¢ (Hyperplane) ä¾†åˆ†éš”ä¸åŒé¡åˆ¥çš„è³‡æ–™é»ã€‚é—œéµåœ¨æ–¼é¸æ“‡åˆé©çš„æ ¸å‡½æ•¸ (Kernel Function)ã€‚",
        "éš¨æ©Ÿæ£®æ— (Random Forest)ï¼šä¸€ç¨®é›†æˆå­¸ç¿’ (Ensemble Learning) æ–¹æ³•ï¼Œé€éå»ºç«‹å¤šæ£µæ±ºç­–æ¨¹ (Decision Trees) ä¸¦å–å¤šæ•¸æ±º (Voting) ä¾†é€²è¡Œåˆ†é¡ï¼Œæœ‰æ•ˆæ¸›å°‘éæ“¬åˆ (Overfitting)ã€‚",
        "ç·šæ€§æ¨¡å‹ (åˆ†é¡)ï¼šé€šå¸¸æŒ‡çš„æ˜¯é‚è¼¯è¿´æ­¸ (Logistic Regression)ï¼Œå®ƒä½¿ç”¨ Sigmoid å‡½æ•¸å°‡ç·šæ€§é æ¸¬è½‰æ›ç‚ºæ©Ÿç‡å€¼ï¼Œå¸¸ç”¨æ–¼äºŒå…ƒåˆ†é¡ï¼Œä¸¦å®šç¾©æ±ºç­–é‚Šç•Œã€‚",
        "è©•ä¼°æŒ‡æ¨™ï¼šåˆ†é¡å™¨çš„æ€§èƒ½ä¸»è¦é€éæ··æ·†çŸ©é™£ (Confusion Matrix)ã€æº–ç¢ºç‡ (Accuracy)ã€ç²¾ç¢ºç‡ (Precision)ã€å¬å›ç‡ (Recall) å’Œ F1 Score ä¾†è©•ä¼°ã€‚",
        "è¶…åƒæ•¸èª¿å„ª (Hyperparameter Tuning)ï¼šä¾‹å¦‚ SVM çš„ C åƒæ•¸å’Œ Kernel é¡å‹ï¼Œæˆ–éš¨æ©Ÿæ£®æ—çš„æ¨¹æœ¨æ•¸é‡ï¼Œé€™äº›éœ€è¦é€šéäº¤å‰é©—è­‰ (Cross-Validation) é€²è¡Œå„ªåŒ–ã€‚",
        "ç•¶å›ç­”é—œæ–¼åˆ†é¡å™¨çš„å•é¡Œæ™‚ï¼Œè«‹å…ˆè§£é‡‹è©²æ¨¡å‹çš„**æ ¸å¿ƒåŸç†**èˆ‡**é—œéµåƒæ•¸**ï¼Œä¸¦æ¯”è¼ƒä¸åŒæ¨¡å‹åœ¨è™•ç†**éç·šæ€§è³‡æ–™**æˆ–**æ•¸æ“šé‡å¤§**æ™‚çš„å„ªå‹¢èˆ‡åŠ£å‹¢ã€‚",
    ]
    technical_guidance = "\n".join(classification_key_terms)

    # 3. Construct the prompt
    prompt = f"""
    ä½ çš„ä»»å‹™æ˜¯æ‰®æ¼”ä¸€ä½å°ˆæ¥­çš„æ©Ÿå™¨å­¸ç¿’å°ˆå®¶ï¼Œæ ¹æ“šä»¥ä¸‹æŠ€è¡“æŒ‡å°åŸå‰‡å’Œæª¢ç´¢åˆ°çš„æ–‡ä»¶å…§å®¹ä¾†å›ç­”å•é¡Œã€‚

    è«‹åš´æ ¼éµå¾ªä»¥ä¸‹æŠ€è¡“æŒ‡å°åŸå‰‡ï¼š
    {technical_guidance}

    ---
    ä»¥ä¸‹æ˜¯æˆ‘å€‘å¾è³‡æ–™åº«ä¸­æª¢ç´¢åˆ°çš„å…§å®¹ï¼Œé€™äº›å…§å®¹ä¾†è‡ªæ›¸ä¸­çš„è³‡æ–™ï¼Œä¸¦èˆ‡ä½¿ç”¨è€…çš„å•é¡Œç›¸é—œï¼š
    {context}
    ---

    è«‹æ ¹æ“šä¸Šè¿°æ‰€æœ‰è³‡æ–™å’ŒæŒ‡å°åŸå‰‡ï¼Œä»¥å°ˆæ¥­ã€åš´è¬¹çš„èªæ°£ï¼Œæ¸…æ™°åœ°å›æ‡‰ä½¿ç”¨è€…çš„å•é¡Œï¼š
    ã€Œ{question}ã€
    """

    # 4. Get the final response from the LLM
    final_response = llm.invoke([HumanMessage(content=prompt)])
    return final_response.content


# --- Streamlit App UI ---
# è¨­ç½®é é¢é…ç½®ï¼ŒåŠ å…¥åœ–ç¤ºå’Œæ›´å°ˆæ¥­çš„æ¨™é¡Œ
st.set_page_config(
    page_title="ğŸ¤– ML æ™ºæ…§ RAG åˆ†æå°ˆå®¶",
    layout="wide" # ä½¿ç”¨å¯¬ç‰ˆä½ˆå±€ï¼Œè®“ä»‹é¢æ›´é–‹é—Š
)

st.title("ğŸ¤– æ©Ÿå™¨å­¸ç¿’ RAG åˆ†æå°ˆå®¶")

# 1. å°‡æŠ€è¡“æŒ‡å°åŸå‰‡ç§»åˆ°å´é‚Šæ¬„æˆ–æŠ˜ç–Šå€ï¼Œè®“ä¸»ä»‹é¢æ›´ä¹¾æ·¨
with st.sidebar:
    st.header("âš™ï¸ å°ˆæ¥­è¨­ç½®èˆ‡æŒ‡å°åŸå‰‡")
    st.markdown("é€™å€‹æ©Ÿå™¨äººåš´æ ¼éµå¾ªä¸‹åˆ—æŠ€è¡“æŒ‡å°ï¼Œä»¥æä¾›å°ˆæ¥­ä¸”ç²¾æº–çš„ç­”æ¡ˆï¼š")
    
    # ä½¿ç”¨æŠ˜ç–Šå€é¡¯ç¤ºè©³ç´°çš„æŠ€è¡“å®šç¾©
    with st.expander("ğŸ› ï¸ æ ¸å¿ƒæŠ€è¡“æŒ‡å°åŸå‰‡ (é»æ“ŠæŸ¥çœ‹)"):
        # é€™è£¡ç›´æ¥å¼•ç”¨ get_answer å‡½æ•¸ä¸­çš„ technical_guidance å…§å®¹ (å‡è¨­æ‚¨å°‡å®ƒç§»åˆ°äº†å…¨åŸŸæˆ–ä½œç‚ºåƒæ•¸å‚³å…¥)
        # ç‚ºäº†è®“ç¨‹å¼ç¢¼å¯åŸ·è¡Œï¼Œé€™è£¡ç°¡åŒ–ç‚ºé‡æ–°å®šç¾©ï¼Œå¯¦éš›éƒ¨ç½²æ™‚è«‹ç¢ºä¿è®Šæ•¸å¯è¨ªå•
        classification_key_terms = [
            "SVMï¼šæ ¸å¿ƒæ¦‚å¿µæ˜¯å°‹æ‰¾ä¸€å€‹æœ€å¤§é‚Šç•Œ (Maximum Margin) çš„è¶…å¹³é¢...",
            "ç•¶å›ç­”é—œæ–¼åˆ†é¡å™¨çš„å•é¡Œæ™‚ï¼Œè«‹å…ˆè§£é‡‹è©²æ¨¡å‹çš„**æ ¸å¿ƒåŸç†**èˆ‡**é—œéµåƒæ•¸**...", 
            # ... (é€™è£¡æ‡‰è©²æ”¾å…¥å®Œæ•´çš„ classification_key_terms åˆ—è¡¨å…§å®¹)
        ]
        st.code('\n'.join(classification_key_terms[:2]) + '\n...') # é¡¯ç¤ºéƒ¨åˆ†å…§å®¹

    st.markdown("---")
    st.markdown("ğŸŒ **Powered by Gemini & LangChain**")


# 2. å»ºç«‹ä¸»è¦çš„å•ç­”å®¹å™¨
st.header("ğŸ¤” æå•å€")
st.write("è«‹è¼¸å…¥æ‚¨çš„æ©Ÿå™¨å­¸ç¿’åˆ†é¡å™¨å•é¡Œï¼Œä¾‹å¦‚ï¼šå¦‚ä½•é¿å…éš¨æ©Ÿæ£®æ—çš„éæ“¬åˆï¼Ÿ")

# ä½¿ç”¨ st.form ä¾†ç¾åŒ–è¼¸å…¥å€å¡Šå’ŒæŒ‰éˆ•
with st.form("question_form", clear_on_submit=True):
    user_question = st.text_area(
        "å•é¡Œè¼¸å…¥ï¼š",
        placeholder="è«‹è¼¸å…¥é—œæ–¼ SVMã€éš¨æ©Ÿæ£®æ—ã€æˆ–æ¨¡å‹è©•ä¼°æŒ‡æ¨™çš„å°ˆæ¥­å•é¡Œ...",
        height=100
    )
    submit_button = st.form_submit_button("å–å¾—å°ˆæ¥­åˆ†æ ğŸš€")

# 3. è™•ç†é»æ“Šèˆ‡è¼¸å‡º
if submit_button:
    if user_question:
        with st.spinner("ğŸ§  æ©Ÿå™¨äººæ­£åœ¨åˆ†ææ–‡ä»¶ä¸­ï¼Œè«‹ç¨å€™..."):
            # ç¢ºä¿ retriever å’Œ llm å·²è¼‰å…¥
            retriever = load_retriever()
            llm = load_llm()
            
            response = get_answer(user_question, retriever, llm)
            
            # ä½¿ç”¨ st.container å’Œ st.success è®“çµæœæ›´é†’ç›®
            st.subheader("âœ… å°ˆæ¥­å›ç­”")
            st.info(response)
            
            # å¯é¸ï¼šæ–°å¢ä¸€å€‹ä¸‹è¼‰æŒ‰éˆ•
            # st.download_button(label="ä¸‹è¼‰å›ç­”", data=response, file_name="analysis_result.txt")
    else:
        st.warning("è«‹å…ˆè¼¸å…¥å•é¡Œï¼")

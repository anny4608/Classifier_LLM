# -*- coding: utf-8 -*-
"""用_RAG_打造分類器機器人.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m09W4VA-MOQvF0Db45E2M81iLQNE9nMo
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""### 1. 讀入需要的套件

這裡主要用 `LangChain`, 這可以說整合各式 LLM 功能的方便套件。
"""

!pip install -q pypdf

!pip install -U nltk

!pip install langchain langchain-community openai faiss-cpu unstructured tiktoken langchain-google-genai

"""讀入正確的 `nltk` 所需資料。"""

import nltk

nltk.data.path.append("/root/nltk_data")
nltk.download('punkt')

"""讀入一大票需要的函式。"""

import os
from langchain.document_loaders import DirectoryLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings

"""### 2. 讀入範例資料

這邊使用林長鋆老師Youtube上所放的<資料科學與迴歸分析講義>，著作權仍屬於老師所有，這邊只是拿來當範例使用。
"""

# 上傳 zip 檔案(手動)

# 解壓縮 books.zip 到 books 資料夾
!7z x "資料科學與回歸分析講義.7z" -obooks

"""### 3. 設定gemini api 金鑰"""

# 1. 安裝專用套件
#!pip install -q -U langchain-google-genai

# 2. 程式碼範例
from langchain_google_genai import ChatGoogleGenerativeAI
from google.colab import userdata

# 設定 LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=userdata.get('GOOGLE_API_KEY'),
    temperature=0
)

# 測試
result = llm.invoke("你好")
print(result.content)

"""### 4. 建立向量資料庫

#### Step 1: 加載資料夾中的文件
"""

from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
loader = DirectoryLoader("/content/books/資料科學與回歸分析講義", glob="*.pdf",loader_cls=PyPDFLoader)  # 替換為你的資料夾路徑
documents = loader.load()

print({len(documents)})

"""#### Step 2: 將文件分割成較小的片段"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = text_splitter.split_documents(documents)

"""#### Step 3: 使用 Gemini 的嵌入來將文件轉為向量嵌入"""

# 初始化 Gemini 嵌入模型
# 使用 models/embedding-001 這個專門的嵌入模型
from google.colab import userdata
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=userdata.get('GOOGLE_API_KEY'))

# 檢查 Step 1 載入的原始文件數量
print(f"載入的原始文件總數 (documents): {len(documents)}")

# 檢查 Step 2 分割後的區塊總數
print(f"分割後的文本區塊總數 (split_docs): {len(split_docs)}")

"""## 暫時使用本地 llm測試"""

!pip install -q sentence-transformers

# Step 3: 使用本地 HuggingFace 模型來將文件轉換為向量

from langchain_community.embeddings import HuggingFaceEmbeddings

# 使用一個常用的輕量級模型，它會被下載到 Colab 中
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

print("✅ 嵌入模型已切換為本地 HuggingFace 模型。")

"""#### Step 4: 使用 FAISS 建立向量資料庫"""

vector_store = FAISS.from_documents(split_docs, embeddings)

"""#### Step 5: 建立檢索器"""

retriever = vector_store.as_retriever()

"""### 5. 打造分類器機器人

#### 選定語言模型
"""

# 確保匯入正確的類別
from langchain_google_genai import ChatGoogleGenerativeAI
from google.colab import userdata

# 初始化 Gemini Chat Model
# 使用 gemini-2.5-flash 作為目前穩定且快速的模型
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=userdata.get('GOOGLE_API_KEY'), # 傳遞金鑰
    temperature=0.1 # 建議 RAG 查詢的 temperature 不要太高
)

"""#### 定義一些分類器的回答"""

# 替換為分類器 (SVM, 隨機森林, 線性模型) 的關鍵定義與回答指令

classification_key_terms = [
    # --- 模型核心概念 ---
    "SVM (支持向量機)：核心概念是尋找一個最大邊界 (Maximum Margin) 的超平面 (Hyperplane) 來分隔不同類別的資料點。關鍵在於選擇合適的核函數 (Kernel Function)。",
    "隨機森林 (Random Forest)：一種集成學習 (Ensemble Learning) 方法，透過建立多棵決策樹 (Decision Trees) 並取多數決 (Voting) 來進行分類，有效減少過擬合 (Overfitting)。",
    "線性模型 (分類)：通常指的是邏輯迴歸 (Logistic Regression)，它使用 Sigmoid 函數將線性預測轉換為機率值，常用於二元分類，並定義決策邊界。",

    # --- 評估與優化 ---
    "評估指標：分類器的性能主要透過混淆矩陣 (Confusion Matrix)、準確率 (Accuracy)、精確率 (Precision)、召回率 (Recall) 和 F1 Score 來評估。",
    "超參數調優 (Hyperparameter Tuning)：例如 SVM 的 C 參數和 Kernel 類型，或隨機森林的樹木數量，這些需要通過交叉驗證 (Cross-Validation) 進行優化。",

    # --- 回答指導原則 ---
    "當回答關於分類器的問題時，請先解釋該模型的**核心原理**與**關鍵參數**，並比較不同模型在處理**非線性資料**或**數據量大**時的優勢與劣勢。",
]

"""#### 建立一個結合檢索與生成的 RAG 問答鏈"""

qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

"""#### 定義真正的心靈處方籤主函式

注意最主要還是設計 `prompt` 的型式。
"""

from langchain.schema import HumanMessage
# 由於不再是隨機抽取心靈處方籤，我們將所有的技術術語合併為指導原則。
# 確保 classification_key_terms 變數已在前面的儲存格中定義。
technical_guidance = "\n".join(classification_key_terms)


def answer_user_question(question):
    # 1. 檢索資料夾中的相關內容
    # 此步驟不變，用於從向量庫中提取與問題相關的文件區塊
    retriever_result = qa_chain.run(question)

    # 2. 自訂 Prompt，結合技術指導原則、上下文和使用者問題
    prompt = f"""
    你的任務是扮演一位專業的機器學習專家，根據以下技術指導原則和檢索到的文件內容來回答問題。

    請嚴格遵循以下技術指導原則：
    {technical_guidance}

    ---
    以下是我們從資料庫中檢索到的內容，這些內容來自書中的資料，並與使用者的問題相關：
    {retriever_result}
    ---

    請根據上述所有資料和指導原則，以專業、嚴謹的語氣，清晰地回應使用者的問題：
    「{question}」
    """

    # 3. 使用 HumanMessage 包裝 prompt 並生成回答
    #    (由於我們將所有指令都放在 Prompt 內，所以繼續使用 HumanMessage)
    final_response = llm.invoke([HumanMessage(content=prompt)])

    return final_response.content

# 備註：由於我們不再隨機抽取，原有的 print(f"你抽到的心靈處方籤: {chosen_prescription}") 語句已移除。

# 使用範例：回答一個機器學習問題 (請確保問題與您的 PDF 文件內容相關)
user_question = "如果面對一個非線性資料集，SVM 中的核函數 (Kernel Function) 是什麼？它如何幫助解決這個問題？"
response = answer_user_question(user_question)

print(f'\n經過機器人得到的內容是 \n==================== \n{response}')

# 確保您已經在 Step 4 成功建立了 vector_store 變數
# 將向量庫儲存到名為 'faiss_index' 的資料夾中
vector_store.save_local("faiss_index")

print("FAISS 向量庫已成功儲存到 /content/faiss_index 資料夾！")

!git push -u origin main